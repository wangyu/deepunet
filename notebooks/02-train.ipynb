{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import six\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import keras as K\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "import pescador\n",
    "import librosa\n",
    "from jams.util import smkdirs\n",
    "\n",
    "import pumpp\n",
    "import random\n",
    "import h5py\n",
    "import jams\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "seed = random.seed(20180319)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arguments(args):\n",
    "    parser = argparse.ArgumentParser(description=__doc__)\n",
    "\n",
    "    parser.add_argument('--max_samples', dest='max_samples', type=int,\n",
    "                        default=128,\n",
    "                        help='Maximum number of samples to draw per streamer')\n",
    "\n",
    "    parser.add_argument('--patch-duration', dest='duration', type=float,\n",
    "                        default=8.0,\n",
    "                        help='Duration (in seconds) of training patches')\n",
    "\n",
    "    parser.add_argument('--seed', dest='seed', type=int,\n",
    "                        default='20170412',\n",
    "                        help='Seed for the random number generator')\n",
    "\n",
    "    parser.add_argument('--train-streamers', dest='train_streamers', type=int,\n",
    "                        default=1024,\n",
    "                        help='Number of active training streamers')\n",
    "\n",
    "    parser.add_argument('--batch-size', dest='batch_size', type=int,\n",
    "                        default=32,\n",
    "                        help='Size of training batches')\n",
    "\n",
    "    parser.add_argument('--rate', dest='rate', type=int,\n",
    "                        default=8,\n",
    "                        help='Rate of pescador stream deactivation')\n",
    "\n",
    "    parser.add_argument('--epochs', dest='epochs', type=int,\n",
    "                        default=100,\n",
    "                        help='Maximum number of epochs to train for')\n",
    "\n",
    "    parser.add_argument('--epoch-size', dest='epoch_size', type=int,\n",
    "                        default=512,\n",
    "                        help='Number of batches per epoch')\n",
    "\n",
    "    parser.add_argument('--validation-size', dest='validation_size', type=int,\n",
    "                        default=1024,\n",
    "                        help='Number of batches per validation')\n",
    "\n",
    "    parser.add_argument('--early-stopping', dest='early_stopping', type=int,\n",
    "                        default=20,\n",
    "                        help='# epochs without improvement to stop')\n",
    "\n",
    "    parser.add_argument('--reduce-lr', dest='reduce_lr', type=int,\n",
    "                        default=10,\n",
    "                        help='# epochs before reducing learning rate')\n",
    "\n",
    "    parser.add_argument(dest='working', type=str,\n",
    "                        help='Path to working directory')\n",
    "\n",
    "    return parser.parse_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/scratch/yw3004/projects/deepunet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sampler(max_samples, duration, pump, seed):\n",
    "\n",
    "    n_frames = librosa.time_to_frames(duration,\n",
    "                                      sr=pump['stft'].sr,\n",
    "                                      hop_length=pump['stft'].hop_length)\n",
    "    \n",
    "    # Only make this object when building the sampler (during training)\n",
    "    fake_p_stft = pumpp.feature.STFTMag(name='output', sr=pump['stft'].sr, \n",
    "                                        hop_length=pump['stft'].hop_length, n_fft=pump['stft'].n_fft, \n",
    "                                        log=False, conv='tf')\n",
    "    \n",
    "    # Make a sampler using real transformer and the fake one for output\n",
    "    return pumpp.Sampler(max_samples, n_frames, pump['stft'], fake_p_stft, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(fname):\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    def collect(k, v):\n",
    "        if isinstance(v, h5py.Dataset):\n",
    "            data[k] = v.value\n",
    "\n",
    "    with h5py.File(fname, mode='r') as hf:\n",
    "        hf.visititems(collect)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original data generation functions (don't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sampler(fname, sampler):\n",
    "    #'''Generate samples from a specified h5 file'''\n",
    "    for datum in sampler(load_h5(fname)):\n",
    "        yield datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(working, tracks, sampler, n_active, rate, batch_size=32,\n",
    "                   **kwargs):\n",
    "    #'''Generate a data stream from a collection of tracks and a sampler'''\n",
    "\n",
    "    streams = []\n",
    "\n",
    "    for track in tracks:\n",
    "        fname = os.path.join(working,\n",
    "                             os.path.extsep.join([str(track), '.h5']))\n",
    "        streams.append(pescador.Streamer(data_sampler, fname, sampler))\n",
    "\n",
    "    # Send it all to a mux\n",
    "    mux = pescador.StochasticMux(streams, n_active, rate, **kwargs)\n",
    "  \n",
    "    if batch_size == 1:\n",
    "        return mux\n",
    "    else:\n",
    "        return pescador.maps.buffer_stream(mux, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified data generation functions (use these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sampler_2(fname, max_samples, duration, pump, seed ):\n",
    "    #'''Generate samples from a specified h5 file'''\n",
    "    sampler = make_sampler(max_samples, duration, pump, seed)\n",
    "\n",
    "    for datum in sampler(load_h5(fname)):\n",
    "        #print(fname)\n",
    "        #print(duration)\n",
    "        \n",
    "        #remove the first dimension of arrays \n",
    "        datum['stft/mag'] = np.squeeze(datum['stft/mag'], axis=(0,))\n",
    "        datum['output/mag'] = np.squeeze(datum['output/mag'], axis=(0,))\n",
    "        yield datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_2(working, tracks, max_samples, duration, pump, seed, n_active, rate, batch_size=32,\n",
    "                   **kwargs):\n",
    "    #'''Generate a data stream from a collection of tracks and a sampler'''\n",
    "\n",
    "    streams = []\n",
    "\n",
    "    for track in tracks:\n",
    "        fname = os.path.join(working,\n",
    "                             os.path.extsep.join([str(track), 'h5']))\n",
    "        streams.append(pescador.Streamer(data_sampler_2, fname, max_samples, duration, pump, seed))\n",
    "\n",
    "    # Send it all to a mux\n",
    "    mux = pescador.StochasticMux(streams, n_active, rate, **kwargs)\n",
    "\n",
    "    if batch_size == 1:\n",
    "        return mux\n",
    "    else:\n",
    "        return pescador.buffer_stream(mux, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training index json files (run ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = jams.util.find_with_extension('/scratch/yw3004/projects/deepunet/pump/000/', 'h5')\n",
    "index_train = {}\n",
    "index_train['id'] = {}\n",
    "iteration = 0\n",
    "for ind in inds:\n",
    "    index_train['id'][iteration] = os.path.basename(ind)[:6]\n",
    "    iteration += 1\n",
    "\n",
    "with open('index_train.json', 'w') as fp:\n",
    "    json.dump(index_train, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model(pump):\n",
    "\n",
    "    model_inputs = 'stft/mag'\n",
    "\n",
    "    # Build the input layer\n",
    "    x = pump.layers()[model_inputs]\n",
    "    x_cropped = K.layers.Cropping2D(cropping=((0, 0), (1, 0)), data_format='channels_last')(x)\n",
    "    print(x._keras_shape)\n",
    "\n",
    "    #Apply batch normalization  #torch model does not have this layer\n",
    "    x_bn = K.layers.BatchNormalization()(x_cropped)\n",
    "\n",
    "    # First convolutional layer: 16 5x5\n",
    "    conv1 = K.layers.Convolution2D(16, (5, 5), strides=2, \n",
    "                                   padding='same',\n",
    "                                   activation='linear', \n",
    "                                   data_format='channels_last')(x_bn)\n",
    "    batch1 = K.layers.BatchNormalization()(conv1) #no batch normalize in first layer, don't know why\n",
    "    lrelu1 = K.layers.LeakyReLU(alpha=0.2)(batch1)\n",
    "    print(lrelu1._keras_shape)\n",
    "    \n",
    "    # Second convolutional layer:\n",
    "    conv2 = K.layers.Convolution2D(32, (5, 5), strides=2, \n",
    "                                   padding='same', \n",
    "                                   activation='linear',\n",
    "                                   data_format='channels_last')(lrelu1)\n",
    "    batch2 = K.layers.BatchNormalization()(conv2)\n",
    "    lrelu2 = K.layers.LeakyReLU(alpha=0.2)(batch2)\n",
    "    print(lrelu2._keras_shape)\n",
    "\n",
    "    # Third convolutional layer: \n",
    "    conv3 = K.layers.Convolution2D(64, (5, 5), strides=2, \n",
    "                                   padding='same', \n",
    "                                   activation='linear',\n",
    "                                   data_format='channels_last')(lrelu2)\n",
    "    batch3 = K.layers.BatchNormalization()(conv3)\n",
    "    lrelu3 = K.layers.LeakyReLU(alpha=0.2)(batch3)\n",
    "    print(lrelu3._keras_shape)\n",
    "\n",
    "\n",
    "    # Fourth convolutional layer: \n",
    "    conv4 = K.layers.Convolution2D(128, (5, 5), strides=2, \n",
    "                                   padding='same', \n",
    "                                   activation='linear',\n",
    "                                   data_format='channels_last')(lrelu3) \n",
    "    batch4 = K.layers.BatchNormalization()(conv4)\n",
    "    lrelu4 = K.layers.LeakyReLU(alpha=0.2)(batch4)\n",
    "    print(lrelu4._keras_shape)\n",
    "\n",
    "\n",
    "    # Fifth convolutional layer: \n",
    "    conv5 = K.layers.Convolution2D(256, (5, 5), strides=2, \n",
    "                                   padding='same', \n",
    "                                   activation='linear',\n",
    "                                   data_format='channels_last')(lrelu4)\n",
    "    batch5 = K.layers.BatchNormalization()(conv5)\n",
    "    lrelu5 = K.layers.LeakyReLU(alpha=0.2)(batch5)\n",
    "    print(lrelu5._keras_shape)\n",
    "    \n",
    "    # Sixth convolutional layer: no batch normalization, regular relu\n",
    "    conv6 = K.layers.Convolution2D(512, (5, 5), strides=2, \n",
    "                                   padding='same', \n",
    "                                   activation='relu',\n",
    "                                   data_format='channels_last')(lrelu5)\n",
    "    print(conv6._keras_shape)\n",
    "    \n",
    "    \n",
    "    # First deconvolutional layer:\n",
    "    deconv7 = K.layers.Conv2DTranspose(256, (5, 5), strides=2, \n",
    "                                       padding='same', \n",
    "                                       activation='linear',\n",
    "                                       data_format='channels_last')(conv6)\n",
    "    batch7 = K.layers.BatchNormalization()(deconv7)\n",
    "    dropout7 = K.layers.Dropout(0.5)(batch7)\n",
    "    concat7 = K.layers.concatenate([dropout7, conv5], axis=3)\n",
    "    relu7 = K.layers.Activation('relu')(concat7)\n",
    "    print(relu7._keras_shape)\n",
    "    \n",
    "    # Second deconvolutional layer:\n",
    "    deconv8 = K.layers.Conv2DTranspose(128, (5, 5), strides=2, \n",
    "                                       padding='same', \n",
    "                                       activation='linear',\n",
    "                                       data_format='channels_last')(relu7)\n",
    "    batch8 = K.layers.BatchNormalization()(deconv8)\n",
    "    dropout8 = K.layers.Dropout(0.5)(batch8)\n",
    "    concat8 = K.layers.concatenate([dropout8, conv4], axis=3)\n",
    "    relu8 = K.layers.Activation('relu')(concat8)\n",
    "    print(relu8._keras_shape)\n",
    "    \n",
    "    # Third deconvolutional layer:\n",
    "    deconv9 = K.layers.Conv2DTranspose(64, (5, 5), strides=2, \n",
    "                                       padding='same', \n",
    "                                       activation='linear',\n",
    "                                       data_format='channels_last')(relu8)\n",
    "    batch9 = K.layers.BatchNormalization()(deconv9)\n",
    "    dropout9 = K.layers.Dropout(0.5)(batch9)\n",
    "    concat9 = K.layers.concatenate([dropout9, conv3], axis=3)\n",
    "    relu9 = K.layers.Activation('relu')(concat9)\n",
    "    print(relu9._keras_shape)\n",
    "    \n",
    "    # Fourth deconvolutional layer:\n",
    "    deconv10 = K.layers.Conv2DTranspose(32, (5, 5), strides=2, \n",
    "                                       padding='same', \n",
    "                                       activation='linear',\n",
    "                                       data_format='channels_last')(relu9)\n",
    "    batch10 = K.layers.BatchNormalization()(deconv10)\n",
    "    concat10 = K.layers.concatenate([batch10, conv2], axis=3)\n",
    "    relu10 = K.layers.Activation('relu')(concat10)\n",
    "    print(relu10._keras_shape)\n",
    "    \n",
    "    # Fifth deconvolutional layer:\n",
    "    deconv11 = K.layers.Conv2DTranspose(16, (5, 5), strides=2, \n",
    "                                       padding='same', \n",
    "                                       activation='linear',\n",
    "                                       data_format='channels_last')(relu10)\n",
    "    batch11 = K.layers.BatchNormalization()(deconv11)\n",
    "    concat11 = K.layers.concatenate([batch11, conv1], axis=3)\n",
    "    relu11 = K.layers.Activation('relu')(concat11)\n",
    "    print(relu11._keras_shape)\n",
    "\n",
    "    # Last deconolutional layer: Mask layer\n",
    "    deconv12 = K.layers.Conv2DTranspose(1, (5, 5), strides=2, \n",
    "                                       padding='same', \n",
    "                                       activation='sigmoid',\n",
    "                                       data_format='channels_last')(relu11)\n",
    "    print(deconv12._keras_shape)\n",
    "    \n",
    "    # Final output: should be pointwise multiplication\n",
    "    outputs = K.layers.Multiply()([x_cropped, deconv12])\n",
    "    print(outputs._keras_shape)\n",
    "    \n",
    "    padded_outputs = K.layers.ZeroPadding2D(padding=((0, 0),(1, 0)), data_format='channels_last')(outputs)\n",
    "    print(padded_outputs._keras_shape)\n",
    "\n",
    "    model = K.models.Model(x, padded_outputs)\n",
    "    model_outputs = 'output/mag'\n",
    "\n",
    "    return model, [model_inputs], [model_outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(working, max_samples, duration, rate,\n",
    "          batch_size, epochs, epoch_size, validation_size,\n",
    "          early_stopping, reduce_lr, seed):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    working : str\n",
    "        directory that contains the experiment data (h5)\n",
    "    max_samples : int\n",
    "        Maximum number of samples per streamer\n",
    "    duration : float\n",
    "        Duration of training patches\n",
    "    batch_size : int\n",
    "        Size of batches\n",
    "    rate : int\n",
    "        Poisson rate for pescador\n",
    "    epochs : int\n",
    "        Maximum number of epoch\n",
    "    epoch_size : int\n",
    "        Number of batches per epoch\n",
    "    validation_size : int\n",
    "        Number of validation batches\n",
    "    early_stopping : int\n",
    "        Number of epochs before early stopping\n",
    "    reduce_lr : int\n",
    "        Number of epochs before reducing learning rate\n",
    "    seed : int\n",
    "        Random seed\n",
    "    '''\n",
    "\n",
    "    # Load the pump\n",
    "    with open(os.path.join(OUTPUT_PATH, 'pump.pkl'), 'rb') as fd:\n",
    "        pump = pickle.load(fd)\n",
    "\n",
    "    # Build the sampler\n",
    "    sampler = make_sampler(max_samples, duration, pump, seed)\n",
    "\n",
    "    # Build the model\n",
    "    model, inputs, outputs = construct_model(pump)\n",
    "\n",
    "    # Load the training data\n",
    "    with open('index_train.json', 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "        idx_train_ = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    # Split the training data into train and validation\n",
    "    splitter_tv = ShuffleSplit(n_splits=1, test_size=0.25,\n",
    "                               random_state=seed)\n",
    "    train, val = next(splitter_tv.split(idx_train_))\n",
    "\n",
    "    idx_train = idx_train_.iloc[train]\n",
    "    idx_val = idx_train_.iloc[val]\n",
    "\n",
    "    gen_train = data_generator_2(working,\n",
    "                               idx_train['id'].values, max_samples, duration, pump, seed, epoch_size,\n",
    "                               rate=rate,\n",
    "                               batch_size=batch_size,\n",
    "                               #revive=True, #what is this doing?\n",
    "                               random_state=seed)\n",
    "\n",
    "\n",
    "    gen_train = pescador.maps.keras_tuples(gen_train, inputs=inputs, outputs=outputs)\n",
    "\n",
    "    gen_val = data_generator_2(working,\n",
    "                             idx_val['id'].values, max_samples, duration, pump, seed, len(idx_val),\n",
    "                             rate=rate,\n",
    "                             batch_size=batch_size,\n",
    "                             #revive=True,\n",
    "                             random_state=seed)\n",
    "\n",
    "\n",
    "    gen_val = pescador.maps.keras_tuples(gen_val, inputs=inputs, outputs=outputs)\n",
    "\n",
    "    \n",
    "    #loss:mean_absolute_error, metric: put loss here\n",
    "    model.compile(K.optimizers.Adam(), loss='mean_absolute_error')\n",
    "\n",
    "    # Store the model\n",
    "    model_spec = K.utils.serialize_keras_object(model)\n",
    "    with open(os.path.join(OUTPUT_PATH, 'model_spec.pkl'), 'wb') as fd:\n",
    "        pickle.dump(model_spec, fd)\n",
    "\n",
    "    # Construct the weight path\n",
    "    weight_path = os.path.join(OUTPUT_PATH, 'model.h5')\n",
    "\n",
    "    # Build the callbacks\n",
    "    #monitor = 'val_loss' #might not need to specify this\n",
    "    cb = []\n",
    "    cb.append(K.callbacks.ModelCheckpoint(weight_path,\n",
    "                                          save_best_only=True,\n",
    "                                          verbose=1,\n",
    "                                          ))\n",
    "\n",
    "    cb.append(K.callbacks.ReduceLROnPlateau(patience=reduce_lr,\n",
    "                                            verbose=1,\n",
    "                                            ))\n",
    "\n",
    "    cb.append(K.callbacks.EarlyStopping(patience=early_stopping,\n",
    "                                        verbose=1,\n",
    "                                       ))\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit_generator(gen_train, epoch_size, epochs,\n",
    "                        validation_data=gen_val,\n",
    "                        validation_steps=validation_size,\n",
    "                        callbacks=cb)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     params = process_arguments(sys.argv[1:])\n",
    "\n",
    "#     #smkdirs(OUTPUT_PATH)\n",
    "\n",
    "#     #version = increment_version(os.path.join(OUTPUT_PATH, 'version.txt'))\n",
    "\n",
    "#     print('{}: training'.format(__doc__))\n",
    "# #    print('Model version: {}'.format(version))\n",
    "#     print(params)\n",
    "\n",
    "#     train(params.working,\n",
    "#           params.max_samples, params.duration,\n",
    "#           params.rate,\n",
    "#           params.batch_size,\n",
    "#           params.epochs, params.epoch_size,\n",
    "#           params.validation_size,\n",
    "#           params.early_stopping,\n",
    "#           params.reduce_lr,\n",
    "#           params.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 513, 1)\n",
      "(None, None, 256, 16)\n",
      "(None, None, 128, 32)\n",
      "(None, None, 64, 64)\n",
      "(None, None, 32, 128)\n",
      "(None, None, 16, 256)\n",
      "(None, None, 8, 512)\n",
      "(None, None, 16, 512)\n",
      "(None, None, 32, 256)\n",
      "(None, None, 64, 128)\n",
      "(None, None, 128, 64)\n",
      "(None, None, 256, 32)\n",
      "(None, None, 512, 1)\n",
      "(None, None, 512, 1)\n",
      "(None, None, 513, 1)\n",
      "Epoch 1/100\n",
      "512/512 [==============================] - 103s 202ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00475, saving model to /scratch/yw3004/projects/deepunet/model.h5\n",
      "Epoch 2/100\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0044"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "low >= high",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-447dbd8814f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/scratch/yw3004/projects/deepunet/pump/000/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-d40def4f1c66>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(working, max_samples, duration, rate, batch_size, epochs, epoch_size, validation_size, early_stopping, reduce_lr, seed)\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                         callbacks=cb)\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2242\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m                                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m                                 max_queue_size=max_queue_size)\n\u001b[0m\u001b[1;32m   2245\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m                     raise ValueError('Output of generator should be a tuple '\n",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m_data_generator_task\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m                             \u001b[0;31m# => Serialize calls to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                             \u001b[0;31m# infinite iterator/generator's next() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                             \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/pescador/maps.py\u001b[0m in \u001b[0;36mkeras_tuples\u001b[0;34m(stream, inputs, outputs)\u001b[0m\n\u001b[1;32m    155\u001b[0m                             '`inputs` or `outputs`')\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/pescador/maps.py\u001b[0m in \u001b[0;36mbuffer_stream\u001b[0;34m(stream, buffer_size, partial)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/pescador/mux.py\u001b[0m in \u001b[0;36miterate\u001b[0;34m(self, max_iter)\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                     \u001b[0;31m# Then yield the sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_mux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreams_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m                     \u001b[0;31m# Increment the sample counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/pescador/core.py\u001b[0m in \u001b[0;36miterate\u001b[0;34m(self, max_iter)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Use self as context manager / calls __enter__() => _activate()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mactive_streamer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_streamer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-d3b38cf7815d>\u001b[0m in \u001b[0;36mdata_sampler_2\u001b[0;34m(fname, max_samples, duration, pump, seed)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_h5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m#print(fname)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#print(duration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/pumpp/sampler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepunet/lib/python3.6/site-packages/pumpp/sampler.py\u001b[0m in \u001b[0;36mindices\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# Generate a sampling interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: low >= high"
     ]
    }
   ],
   "source": [
    "train('/scratch/yw3004/projects/deepunet/pump/000/', 1, 6.0, 8, 32, 100, 512, 10, 20, 10, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepunet",
   "language": "python",
   "name": "deepunet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
